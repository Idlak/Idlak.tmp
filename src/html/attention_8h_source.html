<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<title>Kaldi: nnet3/attention.h Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="icon" href="favicon.ico" type="image/x-icon" />
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" /> 
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
 <td id="projectlogo"><a href="http://kaldi-asr.org/"><img alt="Logo" src="KaldiTextAndLogoSmall.png"/ style="padding: 3px 5px 1px 5px"></a></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname" style="display:none">Kaldi
   </div>
  </td>
    <td style="padding-left: 0.5em;">
    <div id="projectbrief" style="display:none"></div>
    </td>
   <!--END PROJECT_BRIEF-->
  <!--END !PROJECT_NAME-->
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('attention_8h_source.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">attention.h</div>  </div>
</div><!--header-->
<div class="contents">
<a href="attention_8h.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="comment">// nnet3/attention.h</span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;<span class="comment">// Copyright      2017  Johns Hopkins University (author: Daniel Povey)</span></div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment">//                      Hossein Hadian</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment">// See ../../COPYING for clarification regarding multiple authors</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="comment">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment">// you may not use this file except in compliance with the License.</span></div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="comment">// You may obtain a copy of the License at</span></div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="comment">//  http://www.apache.org/licenses/LICENSE-2.0</span></div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="comment">// THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span></div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="comment">// KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED</span></div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="comment">// WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,</span></div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="comment">// MERCHANTABLITY OR NON-INFRINGEMENT.</span></div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;<span class="comment">// See the Apache 2 License for the specific language governing permissions and</span></div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="comment">// limitations under the License.</span></div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;<span class="preprocessor">#ifndef KALDI_NNET3_ATTENTION_H_</span></div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;<span class="preprocessor">#define KALDI_NNET3_ATTENTION_H_</span></div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="kaldi-common_8h.html">base/kaldi-common.h</a>&quot;</span></div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="common-utils_8h.html">util/common-utils.h</a>&quot;</span></div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="options-itf_8h.html">itf/options-itf.h</a>&quot;</span></div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="matrix-lib_8h.html">matrix/matrix-lib.h</a>&quot;</span></div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="cu-matrix-lib_8h.html">cudamatrix/cu-matrix-lib.h</a>&quot;</span></div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="nnet-common_8h.html">nnet3/nnet-common.h</a>&quot;</span></div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="convolution_8h.html">nnet3/convolution.h</a>&quot;</span></div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;<span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;<span class="keyword">namespace </span><a class="code" href="namespacekaldi.html">kaldi</a> {</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;<span class="keyword">namespace </span>nnet3 {</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;<span class="keyword">namespace </span>attention {</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;<span class="comment">// Our attention is &quot;multi-head&quot;, like in Google&#39;s paper.  Note: we&#39;re basically</span></div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;<span class="comment">// implementing multi-head attention as a fixed nonlinearity, with the actual</span></div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;<span class="comment">// parameters relegated to the previous layer.  That is, the attention layer</span></div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;<span class="comment">// won&#39;t have any parameters of its own, but the parameters of the preceding</span></div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;<span class="comment">// layer will be interpretable as the parameters.  It doesn&#39;t change what&#39;s</span></div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;<span class="comment">// computed, it just affects how the neural net is divided into components.</span></div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;<span class="comment">//  * Basic restricted self-attention (without positional encoding).</span></div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;<span class="comment">// To explain what&#39;s going on, we start with the simplest form of attention:</span></div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;<span class="comment">// single-head, and no positional encoding, but with restricted context.  For purposes</span></div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;<span class="comment">// of exposition we assume that the time offsets we need form a contigous</span></div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;<span class="comment">// range, i.e. with time-stride == 1; the code does have the notion of a stride (you&#39;ll</span></div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;<span class="comment">// see later).</span></div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;<span class="comment">// Using notation similar to the Google paper, suppose we have a time-sequence</span></div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;<span class="comment">// of inputs, and the inputs are (keys, values and queries):</span></div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;<span class="comment">//   k_t, v_t, q_t</span></div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;<span class="comment">// where k_t and q_t are vectors of dimension &#39;key_dim&#39; and v_t is a vector</span></div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;<span class="comment">// of dimension &#39;value_dim&#39; (you may choose to make this the same as key_dim, but</span></div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;<span class="comment">// that isn&#39;t a constraint).</span></div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;<span class="comment">// Let&#39;s make num_left_inputs and num_right_inputs be the number of</span></div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;<span class="comment">// left-context and right-context frames required, and for some t,</span></div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;<span class="comment">// let input_indexes(t) be the set</span></div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;<span class="comment">//  [ t - num_left_inputs, t - num_left_inputs + 1, ... t + num_right_inputs].</span></div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;<span class="comment">// To evaluate the output (which we&#39;ll write u_t), we need the query</span></div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;<span class="comment">// value q_t, plus the keys and values k_s and v_s for all s in input_indexes(t).</span></div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;<span class="comment">// If the inputs are not available for some subset of input_indexes(t),</span></div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;<span class="comment">// we just let them be zeros; the network can learn to ignore them if it wants,</span></div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;<span class="comment">// but making them zeros is simpler to implement.</span></div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;<span class="comment">// Anyway, the output u_t (without positional encoding yet) is:</span></div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;<span class="comment">//  u_t := \sum_{s in input_indexes(t)}  Z_t exp(q_t . k_s) v_s</span></div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;<span class="comment">// where Z_t is 1/(\sum_s exp(q_t . k_s)).  We&#39;ll handle scaling</span></div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;<span class="comment">// issues (the 1/sqrt(dim) factor in the Google paper) later on,</span></div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;<span class="comment">// by scaling the keys.</span></div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;<span class="comment">// * Positional encoding</span></div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;<span class="comment">// We now explain how we include positional encoding in the model.</span></div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;<span class="comment">// Let context_dim = 1 + num_left_inputs + num_right_inputs.</span></div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;<span class="comment">// Let v be a vector, and let the function Extend(v, o) (where</span></div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;<span class="comment">// 0 &lt;= o &lt; context_dim) extend v with extra dimensions</span></div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;<span class="comment">// that encode the time-offset.  To be precise, we have</span></div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;<span class="comment">//  Extend(v, o) = Append(v, u_o)</span></div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;<span class="comment">// where u_o is a unit vector of dimension context_dim that is nonzero in the</span></div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;<span class="comment">// o&#39;th dimension (assuming zero-based indexing).</span></div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;<span class="comment">// So when we add the positional encoding (and the scale on the keys), we replace</span></div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;<span class="comment">// the equation:</span></div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;<span class="comment">//  u_t := \sum_{s in input_indexes(t)}  Z_t exp(q_t . k_s) v_s</span></div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;<span class="comment">// with:</span></div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;<span class="comment">//  u_t := \sum_{s in input_indexes(t)}  Z_t exp(alpha q_t . Extend(key-scale * k_s, s - t + num_left_inputs)) Extend(v_s, s - t + num_left_inputs)</span></div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;<span class="comment">// (we won&#39;t actually physically extend the vectors as we compute this,</span></div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;<span class="comment">// we&#39;ll do it a different way, but it&#39;s equivalent to what we wrote</span></div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;<span class="comment">// above. The dimension of q_t is key_dim + context_dim, and the dimension</span></div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;<span class="comment">// of the output u_t is value_dim + context_dim.</span></div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;<span class="comment">// * Multi-head attention</span></div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;<span class="comment">// The attention component if we had a single head, would have an input dimension</span></div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;<span class="comment">// of (2*key_dim + context_dim + value_dim), which would be interpreted</span></div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;<span class="comment">// as Append(k_t, q_t, v_t), of dimensions respectively</span></div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;<span class="comment">// (key_dim, key_dim + context_dim, value_dim).  It would have an output</span></div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;<span class="comment">// dimension of value_dim + context_dim.</span></div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;<span class="comment">// In any case, the multi-head version has input and output dimension that</span></div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;<span class="comment">// are larger by a factor of &#39;num_heads&#39;, and which is equivalent to</span></div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;<span class="comment">// several single-head components appended together.</span></div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;<span class="comment">//  * The actual calculation</span></div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;<span class="comment">// Let&#39;s assume that we might have multiple independent sequences; we&#39;ll</span></div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;<span class="comment">// call this &#39;num_images&#39; because we&#39;re borrowing certain structures from</span></div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;<span class="comment">// the convolution code.</span></div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;</div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;<span class="comment">// The input is formatted as a matrix, whose NumRows() could be written as</span></div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;<span class="comment">// num_images * num_t_in, where num_t_in is the number of distinct input &#39;t&#39;</span></div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;<span class="comment">// values, and whose output is num_images * num_t_out.  To keep it simple we&#39;ll</span></div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;<span class="comment">// explain this under the assumption that we don&#39;t have any &#39;t&#39; stride in the</span></div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;<span class="comment">// attention (t_stride == 1 in the code), and that num_heads == 1; both of</span></div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;<span class="comment">// those are fairly simple modifications to the basic scheme.</span></div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;<span class="comment">// The image (normally &#39;n&#39;) index has a higher stride than the &#39;t&#39; index in</span></div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;<span class="comment">// both the input and the output.  We assume that there is &#39;enough&#39;</span></div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;<span class="comment">// context of the input to compute all required offsets of the output.</span></div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;<span class="comment">// Define the intermediate quantity b_{t,o}, which you can think of</span></div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;<span class="comment">// as the input to softmax; the index &#39;t&#39; is the output time-index</span></div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;<span class="comment">// index at the output, and o ranges from 0 to context_dim - 1.</span></div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;<span class="comment">//    b_{t,o} =  q_t . Extend(key-scale * k_{t + o - num_left_inputs}, o)</span></div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;<span class="comment">// To get rid of the Extend() expressions, define sub-ranges of q_t by</span></div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;<span class="comment">// writing q_t = Append(r_t, s_t) where r_t is of dimension &#39;key_dim&#39;</span></div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;<span class="comment">// and s_t is of dimension context_dim.</span></div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;<span class="comment">//    b_{t,o} =   s_{t,o}  +  key-scale (r_t . k_{t+o-num_left_inputs})  [eqn:b]</span></div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;<span class="comment">// The &#39;b&#39; quantity is the input to the softmax.  Define</span></div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;<span class="comment">//     c_t = Softmax(b_t)</span></div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;<span class="comment">// so \sum_o c_{t,o} = 1.0.  These are the weights on the values.</span></div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;<span class="comment">//  The output can be written as:</span></div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;<span class="comment">//        u_t :=  \sum_o c_{t,o} Extend(v_{t+o-num_left_inputs}, o)</span></div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;<span class="comment">//  but we can write this in a form more suitable for computation as:</span></div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;<span class="comment">//     u_t :=  Append(\sum_o c_{t,o} v_{t+o-num_left_inputs},  c_t)     [eqn:u]</span></div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;<span class="comment">//  * Implementation</span></div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;<span class="comment">// The most time-consuming parts of this computation, we imagine, would be the</span></div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;<span class="comment">// dot-products in [eqn:b] and the weighted sum (\sum_o) in [eqn:u].  They have</span></div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;<span class="comment">// an awkward band-diagonal structure that would not be particularly convenient</span></div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;<span class="comment">// to implement using CUBLAS and the like; I don&#39;t believe the relevant operations</span></div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;<span class="comment">// exist in the BLAS interface, at least for [eqn:u].</span></div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;<span class="comment">//</span></div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;<span class="comment">// In the future I hope to implement this with block-wise matrix</span></div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;<span class="comment">// multiplies-- imagine covering the band-diagonal part of a matrix with</span></div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;<span class="comment">// rectangular blocks in such a way that all the nonzero elements are covered,</span></div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;<span class="comment">// but the blocks might go over the zero parts a bit.   This could be done with</span></div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;<span class="comment">// Or perhaps we can figure out how to implement the block-diagonal matrix</span></div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;<span class="comment">// multiplies in CUDA.</span></div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;</div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;</div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;</div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;<span class="keywordtype">void</span> <a class="code" href="namespacekaldi_1_1nnet3_1_1attention.html#ab636ca156e6173e9a0988d0219d8f0ae">GetAttentionDotProducts</a>(<a class="code" href="namespacekaldi.html#aa66ff8367094543664b3b6a13d77c139">BaseFloat</a> alpha,</div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;                             <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;A,</div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;                             <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;B,</div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;                             CuMatrixBase&lt;BaseFloat&gt; *C);</div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;</div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;</div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;<span class="keywordtype">void</span> <a class="code" href="namespacekaldi_1_1nnet3_1_1attention.html#a6e1f40110561ed112389ea70d5991955">ApplyScalesToOutput</a>(<a class="code" href="namespacekaldi.html#aa66ff8367094543664b3b6a13d77c139">BaseFloat</a> alpha,</div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;                         <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;B,</div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;                         <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;C,</div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;                         CuMatrixBase&lt;BaseFloat&gt; *A);</div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;</div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;</div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;<span class="keywordtype">void</span> <a class="code" href="namespacekaldi_1_1nnet3_1_1attention.html#a20b4cc1d7354d0d53c39a18e7664f23a">ApplyScalesToInput</a>(<a class="code" href="namespacekaldi.html#aa66ff8367094543664b3b6a13d77c139">BaseFloat</a> alpha,</div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;                        <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;A,</div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;                        <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;C,</div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;                        CuMatrixBase&lt;BaseFloat&gt; *B);</div><div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;</div><div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;</div><div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;</div><div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;<span class="keywordtype">void</span> <a class="code" href="namespacekaldi_1_1nnet3_1_1attention.html#a4dba6c0cd0571bfe5b1434b30e7d7bdf">AttentionForward</a>(<a class="code" href="namespacekaldi.html#aa66ff8367094543664b3b6a13d77c139">BaseFloat</a> key_scale,</div><div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;                      <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;keys,</div><div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;                      <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;queries,</div><div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;                      <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;values,</div><div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;                      CuMatrixBase&lt;BaseFloat&gt; *c,</div><div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;                      CuMatrixBase&lt;BaseFloat&gt; *output);</div><div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;</div><div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;<span class="keywordtype">void</span> <a class="code" href="namespacekaldi_1_1nnet3_1_1attention.html#a714ba1d18b57b12b94ae67f551cbe522">AttentionBackward</a>(<a class="code" href="namespacekaldi.html#aa66ff8367094543664b3b6a13d77c139">BaseFloat</a> key_scale,</div><div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;                       <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;keys,</div><div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;                       <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;queries,</div><div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;                       <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;values,</div><div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;                       <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;c,</div><div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;                       <span class="keyword">const</span> CuMatrixBase&lt;BaseFloat&gt; &amp;output_deriv,</div><div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;                       CuMatrixBase&lt;BaseFloat&gt; *keys_deriv,</div><div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;                       CuMatrixBase&lt;BaseFloat&gt; *queries_deriv,</div><div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;                       CuMatrixBase&lt;BaseFloat&gt; *values_deriv);</div><div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;</div><div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;</div><div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;</div><div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;</div><div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;</div><div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;</div><div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;} <span class="comment">// namespace attention</span></div><div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;} <span class="comment">// namespace nnet3</span></div><div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;} <span class="comment">// namespace kaldi</span></div><div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;</div><div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;</div><div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;<span class="preprocessor">#endif</span></div><div class="ttc" id="namespacekaldi_html"><div class="ttname"><a href="namespacekaldi.html">kaldi</a></div><div class="ttdoc">Relabels neural network egs with the read pdf-id alignments. </div><div class="ttdef"><b>Definition:</b> <a href="chain_8dox_source.html#l00020">chain.dox:20</a></div></div>
<div class="ttc" id="namespacekaldi_1_1nnet3_1_1attention_html_a714ba1d18b57b12b94ae67f551cbe522"><div class="ttname"><a href="namespacekaldi_1_1nnet3_1_1attention.html#a714ba1d18b57b12b94ae67f551cbe522">kaldi::nnet3::attention::AttentionBackward</a></div><div class="ttdeci">void AttentionBackward(BaseFloat key_scale, const CuMatrixBase&lt; BaseFloat &gt; &amp;keys, const CuMatrixBase&lt; BaseFloat &gt; &amp;queries, const CuMatrixBase&lt; BaseFloat &gt; &amp;values, const CuMatrixBase&lt; BaseFloat &gt; &amp;c, const CuMatrixBase&lt; BaseFloat &gt; &amp;output_deriv, CuMatrixBase&lt; BaseFloat &gt; *keys_deriv, CuMatrixBase&lt; BaseFloat &gt; *queries_deriv, CuMatrixBase&lt; BaseFloat &gt; *values_deriv)</div><div class="ttdoc">Performs the backward pass corresponding to &amp;#39;AttentionForward&amp;#39;, propagating the derivative back to th...</div><div class="ttdef"><b>Definition:</b> <a href="attention_8cc_source.html#l00154">attention.cc:154</a></div></div>
<div class="ttc" id="matrix-lib_8h_html"><div class="ttname"><a href="matrix-lib_8h.html">matrix-lib.h</a></div></div>
<div class="ttc" id="nnet-common_8h_html"><div class="ttname"><a href="nnet-common_8h.html">nnet-common.h</a></div></div>
<div class="ttc" id="namespacekaldi_1_1nnet3_1_1attention_html_a20b4cc1d7354d0d53c39a18e7664f23a"><div class="ttname"><a href="namespacekaldi_1_1nnet3_1_1attention.html#a20b4cc1d7354d0d53c39a18e7664f23a">kaldi::nnet3::attention::ApplyScalesToInput</a></div><div class="ttdeci">void ApplyScalesToInput(BaseFloat alpha, const CuMatrixBase&lt; BaseFloat &gt; &amp;A, const CuMatrixBase&lt; BaseFloat &gt; &amp;C, CuMatrixBase&lt; BaseFloat &gt; *B)</div><div class="ttdoc">This function is related to GetAttentionDotProducts(); it is used in backprop. </div><div class="ttdef"><b>Definition:</b> <a href="attention_8cc_source.html#l00076">attention.cc:76</a></div></div>
<div class="ttc" id="common-utils_8h_html"><div class="ttname"><a href="common-utils_8h.html">common-utils.h</a></div></div>
<div class="ttc" id="namespacekaldi_1_1nnet3_1_1attention_html_a4dba6c0cd0571bfe5b1434b30e7d7bdf"><div class="ttname"><a href="namespacekaldi_1_1nnet3_1_1attention.html#a4dba6c0cd0571bfe5b1434b30e7d7bdf">kaldi::nnet3::attention::AttentionForward</a></div><div class="ttdeci">void AttentionForward(BaseFloat key_scale, const CuMatrixBase&lt; BaseFloat &gt; &amp;keys, const CuMatrixBase&lt; BaseFloat &gt; &amp;queries, const CuMatrixBase&lt; BaseFloat &gt; &amp;values, CuMatrixBase&lt; BaseFloat &gt; *c, CuMatrixBase&lt; BaseFloat &gt; *output)</div><div class="ttdoc">This is a higher-level interface to the attention code. </div><div class="ttdef"><b>Definition:</b> <a href="attention_8cc_source.html#l00097">attention.cc:97</a></div></div>
<div class="ttc" id="options-itf_8h_html"><div class="ttname"><a href="options-itf_8h.html">options-itf.h</a></div></div>
<div class="ttc" id="namespacekaldi_html_aa66ff8367094543664b3b6a13d77c139"><div class="ttname"><a href="namespacekaldi.html#aa66ff8367094543664b3b6a13d77c139">kaldi::BaseFloat</a></div><div class="ttdeci">float BaseFloat</div><div class="ttdef"><b>Definition:</b> <a href="kaldi-types_8h_source.html#l00029">kaldi-types.h:29</a></div></div>
<div class="ttc" id="convolution_8h_html"><div class="ttname"><a href="convolution_8h.html">convolution.h</a></div><div class="ttdoc">This file contains some fairly low-level utilities for implementing convolutional neural networks and...</div></div>
<div class="ttc" id="namespacekaldi_1_1nnet3_1_1attention_html_a6e1f40110561ed112389ea70d5991955"><div class="ttname"><a href="namespacekaldi_1_1nnet3_1_1attention.html#a6e1f40110561ed112389ea70d5991955">kaldi::nnet3::attention::ApplyScalesToOutput</a></div><div class="ttdeci">void ApplyScalesToOutput(BaseFloat alpha, const CuMatrixBase&lt; BaseFloat &gt; &amp;B, const CuMatrixBase&lt; BaseFloat &gt; &amp;C, CuMatrixBase&lt; BaseFloat &gt; *A)</div><div class="ttdoc">This function is related to GetAttentionDotProducts(); it is used in scaling the values by the softma...</div><div class="ttdef"><b>Definition:</b> <a href="attention_8cc_source.html#l00055">attention.cc:55</a></div></div>
<div class="ttc" id="namespacekaldi_1_1nnet3_1_1attention_html_ab636ca156e6173e9a0988d0219d8f0ae"><div class="ttname"><a href="namespacekaldi_1_1nnet3_1_1attention.html#ab636ca156e6173e9a0988d0219d8f0ae">kaldi::nnet3::attention::GetAttentionDotProducts</a></div><div class="ttdeci">void GetAttentionDotProducts(BaseFloat alpha, const CuMatrixBase&lt; BaseFloat &gt; &amp;A, const CuMatrixBase&lt; BaseFloat &gt; &amp;B, CuMatrixBase&lt; BaseFloat &gt; *C)</div><div class="ttdoc">This function is a utility function that is at the core of how we implement attention. </div><div class="ttdef"><b>Definition:</b> <a href="attention_8cc_source.html#l00032">attention.cc:32</a></div></div>
<div class="ttc" id="cu-matrix-lib_8h_html"><div class="ttname"><a href="cu-matrix-lib_8h.html">cu-matrix-lib.h</a></div></div>
<div class="ttc" id="kaldi-common_8h_html"><div class="ttname"><a href="kaldi-common_8h.html">kaldi-common.h</a></div></div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="dir_67a0a17020b8eedc08ffa5626b1d53a1.html">nnet3</a></li><li class="navelem"><a class="el" href="attention_8h.html">attention.h</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
